{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Is38Ng6Ub2Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking, Modelling, Traning, Predict"
      ],
      "metadata": {
        "id": "XYfK4BFKcEUU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24UISEEvbxrz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: CUDA not available! Will use CPU (very slow)\")\n",
        "\n",
        "# ==================== STEP 2: Dataset Analysis ====================\n",
        "def analyze_dataset(mask_dir, image_dir):\n",
        "    \"\"\"Analyze dataset to detect number of classes and mask format\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ANALYZING DATASET...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    mask_files = sorted(glob(os.path.join(mask_dir, \"*.png\")))\n",
        "    image_files = sorted(glob(os.path.join(image_dir, \"*.png\")))\n",
        "\n",
        "    print(f\"Found {len(mask_files)} mask files\")\n",
        "    print(f\"Found {len(image_files)} image files\")\n",
        "\n",
        "    if len(mask_files) == 0 or len(image_files) == 0:\n",
        "        raise ValueError(\"No images found! Check your paths.\")\n",
        "\n",
        "    # Analyze first mask\n",
        "    sample_mask = np.array(Image.open(mask_files[0]))\n",
        "    print(f\"\\nMask shape: {sample_mask.shape}\")\n",
        "    print(f\"Mask dtype: {sample_mask.dtype}\")\n",
        "\n",
        "    # Check if RGB or Grayscale\n",
        "    if len(sample_mask.shape) == 3:\n",
        "        print(f\"Mask format: RGB (channels: {sample_mask.shape[2]})\")\n",
        "        unique_colors = np.unique(sample_mask.reshape(-1, sample_mask.shape[2]), axis=0)\n",
        "        num_classes = len(unique_colors)\n",
        "        print(f\"Unique colors found: {num_classes}\")\n",
        "        print(f\"Colors (first 10): \\n{unique_colors[:10]}\")\n",
        "        is_rgb = True\n",
        "        unique_values = None\n",
        "    else:\n",
        "        print(f\"Mask format: Grayscale\")\n",
        "        unique_values = np.unique(sample_mask)\n",
        "        num_classes = int(unique_values.max()) + 1\n",
        "        print(f\"Unique values: {unique_values}\")\n",
        "        print(f\"Max class value: {unique_values.max()}\")\n",
        "        print(f\"Number of classes (max+1): {num_classes}\")\n",
        "        is_rgb = False\n",
        "\n",
        "    # Sample image\n",
        "    sample_image = np.array(Image.open(image_files[0]))\n",
        "    print(f\"\\nImage shape: {sample_image.shape}\")\n",
        "    print(f\"Image dtype: {sample_image.dtype}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return {\n",
        "        'num_classes': num_classes,\n",
        "        'is_rgb_mask': is_rgb,\n",
        "        'mask_files': mask_files,\n",
        "        'image_files': image_files,\n",
        "        'image_shape': sample_image.shape,\n",
        "        'mask_shape': sample_mask.shape,\n",
        "        'unique_values': unique_values\n",
        "    }\n",
        "\n",
        "# ==================== STEP 3: Custom Dataset ====================\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, num_classes, is_rgb_mask=False,\n",
        "                 img_size=(256, 256), augment=False, value_map=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.num_classes = num_classes\n",
        "        self.is_rgb_mask = is_rgb_mask\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.value_map = value_map\n",
        "\n",
        "        if is_rgb_mask:\n",
        "            self.color_map = self._build_color_map()\n",
        "\n",
        "    def _build_color_map(self):\n",
        "        \"\"\"Build mapping from RGB colors to class indices\"\"\"\n",
        "        color_map = {}\n",
        "        sample_mask = np.array(Image.open(self.mask_paths[0]))\n",
        "        unique_colors = np.unique(sample_mask.reshape(-1, sample_mask.shape[2]), axis=0)\n",
        "\n",
        "        for idx, color in enumerate(unique_colors):\n",
        "            color_map[tuple(color)] = idx\n",
        "\n",
        "        return color_map\n",
        "\n",
        "    def _rgb_to_class(self, mask):\n",
        "        \"\"\"Convert RGB mask to class indices\"\"\"\n",
        "        h, w = mask.shape[:2]\n",
        "        class_mask = np.zeros((h, w), dtype=np.int64)\n",
        "\n",
        "        for color, class_idx in self.color_map.items():\n",
        "            match = np.all(mask == color, axis=-1)\n",
        "            class_mask[match] = class_idx\n",
        "\n",
        "        return class_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        image = image.resize(self.img_size, Image.BILINEAR)\n",
        "        image = np.array(image) / 255.0\n",
        "\n",
        "        # Load mask\n",
        "        mask = Image.open(self.mask_paths[idx])\n",
        "        mask = mask.resize(self.img_size, Image.NEAREST)\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        if self.is_rgb_mask:\n",
        "            mask = self._rgb_to_class(mask)\n",
        "\n",
        "        if self.value_map is not None:\n",
        "            mask_remapped = np.zeros_like(mask)\n",
        "            for old_val, new_val in self.value_map.items():\n",
        "                mask_remapped[mask == old_val] = new_val\n",
        "            mask = mask_remapped\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = torch.FloatTensor(image).permute(2, 0, 1)  # HWC to CHW\n",
        "        mask = torch.LongTensor(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# ==================== STEP 4: FCN-8s Architecture ====================\n",
        "class FCN8s(nn.Module):\n",
        "    def __init__(self, num_classes=21, pretrained=True):\n",
        "        super(FCN8s, self).__init__()\n",
        "\n",
        "        # Load pretrained VGG16\n",
        "        vgg16 = models.vgg16(pretrained=pretrained)\n",
        "        features = list(vgg16.features.children())\n",
        "\n",
        "        # Encoder (VGG16 feature extractor)\n",
        "        # Pool3: layers 0-16\n",
        "        self.pool3 = nn.Sequential(*features[:17])\n",
        "\n",
        "        # Pool4: layers 17-23\n",
        "        self.pool4 = nn.Sequential(*features[17:24])\n",
        "\n",
        "        # Pool5: layers 24-30\n",
        "        self.pool5 = nn.Sequential(*features[24:])\n",
        "\n",
        "        # FC6 - converted to conv\n",
        "        self.fc6 = nn.Conv2d(512, 4096, kernel_size=7, padding=3)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.drop6 = nn.Dropout2d()\n",
        "\n",
        "        # FC7 - converted to conv\n",
        "        self.fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.drop7 = nn.Dropout2d()\n",
        "\n",
        "        # Score layers\n",
        "        self.score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n",
        "        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "        self.score_pool3 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "        # Upsampling layers\n",
        "        self.upscore2 = nn.ConvTranspose2d(\n",
        "            num_classes, num_classes, kernel_size=4, stride=2, bias=False\n",
        "        )\n",
        "        self.upscore_pool4 = nn.ConvTranspose2d(\n",
        "            num_classes, num_classes, kernel_size=4, stride=2, bias=False\n",
        "        )\n",
        "        self.upscore8 = nn.ConvTranspose2d(\n",
        "            num_classes, num_classes, kernel_size=16, stride=8, bias=False\n",
        "        )\n",
        "\n",
        "        # Initialize upsampling with bilinear interpolation\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights for upsampling layers\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get input size for later cropping\n",
        "        input_size = x.size()[2:]\n",
        "\n",
        "        # Encoder\n",
        "        pool3 = self.pool3(x)  # 1/8\n",
        "        pool4 = self.pool4(pool3)  # 1/16\n",
        "        pool5 = self.pool5(pool4)  # 1/32\n",
        "\n",
        "        # FC layers as convolutions\n",
        "        fc6 = self.relu6(self.fc6(pool5))\n",
        "        fc6 = self.drop6(fc6)\n",
        "\n",
        "        fc7 = self.relu7(self.fc7(fc6))\n",
        "        fc7 = self.drop7(fc7)\n",
        "\n",
        "        # Score\n",
        "        score_fr = self.score_fr(fc7)\n",
        "\n",
        "        # Upsample score_fr (1/32 -> 1/16)\n",
        "        upscore2 = self.upscore2(score_fr)\n",
        "\n",
        "        # Score pool4\n",
        "        score_pool4 = self.score_pool4(pool4)\n",
        "\n",
        "        # Crop and add\n",
        "        upscore2 = self._crop(upscore2, score_pool4)\n",
        "        fuse_pool4 = upscore2 + score_pool4\n",
        "\n",
        "        # Upsample fuse_pool4 (1/16 -> 1/8)\n",
        "        upscore_pool4 = self.upscore_pool4(fuse_pool4)\n",
        "\n",
        "        # Score pool3\n",
        "        score_pool3 = self.score_pool3(pool3)\n",
        "\n",
        "        # Crop and add\n",
        "        upscore_pool4 = self._crop(upscore_pool4, score_pool3)\n",
        "        fuse_pool3 = upscore_pool4 + score_pool3\n",
        "\n",
        "        # Final upsampling (1/8 -> 1/1)\n",
        "        out = self.upscore8(fuse_pool3)\n",
        "\n",
        "        # Crop to input size\n",
        "        out = self._crop(out, x)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _crop(self, input_tensor, target_tensor):\n",
        "        \"\"\"Crop input_tensor to match target_tensor size\"\"\"\n",
        "        _, _, h, w = target_tensor.size()\n",
        "        return input_tensor[:, :, :h, :w]\n",
        "\n",
        "# ==================== STEP 5: Training Functions ====================\n",
        "def train_epoch(model, loader, criterion, optimizer, device, epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_pixels = 0\n",
        "    total_pixels =\n",
        "\n",
        "    from tqdm import tqdm\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{num_epochs} [TRAIN]\")\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(pbar):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct_pixels += (preds == masks).sum().item()\n",
        "        total_pixels += masks.numel()\n",
        "\n",
        "        # Update progress bar\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "        accuracy = 100.0 * correct_pixels / total_pixels\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{avg_loss:.4f}',\n",
        "            'acc': f'{accuracy:.2f}%'\n",
        "        })\n",
        "\n",
        "    return total_loss / len(loader), accuracy\n",
        "\n",
        "def validate(model, loader, criterion, device, epoch, num_epochs):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    from tqdm import tqdm\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{num_epochs} [VAL]  \")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, masks) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct_pixels += (preds == masks).sum().item()\n",
        "            total_pixels += masks.numel()\n",
        "\n",
        "            # Update progress bar\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            accuracy = 100.0 * correct_pixels / total_pixels\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{avg_loss:.4f}',\n",
        "                'acc': f'{accuracy:.2f}%'\n",
        "            })\n",
        "\n",
        "    return total_loss / len(loader), accuracy\n",
        "\n",
        "def calculate_iou(pred, target, num_classes):\n",
        "    \"\"\"Calculate Intersection over Union (IoU)\"\"\"\n",
        "    ious = []\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls)\n",
        "        target_cls = (target == cls)\n",
        "\n",
        "        intersection = (pred_cls & target_cls).sum().float()\n",
        "        union = (pred_cls | target_cls).sum().float()\n",
        "\n",
        "        if union == 0:\n",
        "            iou = float('nan')\n",
        "        else:\n",
        "            iou = intersection / union\n",
        "\n",
        "        ious.append(iou.item())\n",
        "\n",
        "    return np.nanmean(ious)\n",
        "\n",
        "# ==================== STEP 6: Visualization ====================\n",
        "def visualize_predictions(model, dataset, device, num_samples=4):\n",
        "    \"\"\"Visualize model predictions\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 4))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, mask = dataset[idx]\n",
        "\n",
        "            # Predict\n",
        "            image_input = image.unsqueeze(0).to(device)\n",
        "            output = model(image_input)\n",
        "            pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "            # Denormalize image for display\n",
        "            image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "            mask_np = mask.cpu().numpy()\n",
        "\n",
        "            # Plot\n",
        "            axes[i, 0].imshow(image_np)\n",
        "            axes[i, 0].set_title('Original Image')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            axes[i, 1].imshow(mask_np, cmap='tab20')\n",
        "            axes[i, 1].set_title('Ground Truth')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            axes[i, 2].imshow(pred, cmap='tab20')\n",
        "            axes[i, 2].set_title('Prediction')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions_fcn8s.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# ==================== STEP 7: MAIN EXECUTION ====================\n",
        "def main():\n",
        "    global device\n",
        "    # Enable CUDA debugging\n",
        "    import os\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "    # Clear CUDA cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"üîÑ Cleared CUDA cache\\n\")\n",
        "\n",
        "    # CONFIGURE THESE PATHS\n",
        "    MASK_DIR = \"/content/drive/MyDrive/datasets/masks\"\n",
        "    IMAGE_DIR = \"/content/drive/MyDrive/datasets/images\"\n",
        "\n",
        "    # Hyperparameters\n",
        "    IMG_SIZE = (256, 256)\n",
        "    BATCH_SIZE = 4  # FCN8s uses more memory, reduce batch size\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 1e-4\n",
        "\n",
        "    # Step 1: Analyze dataset\n",
        "    info = analyze_dataset(MASK_DIR, IMAGE_DIR)\n",
        "    num_classes = info['num_classes']\n",
        "    is_rgb_mask = info['is_rgb_mask']\n",
        "\n",
        "    print(f\"\\nDetected {num_classes} classes\")\n",
        "    print(f\"Mask format: {'RGB' if is_rgb_mask else 'Grayscale'}\")\n",
        "\n",
        "    # Create value mapping for grayscale masks\n",
        "    value_map = None\n",
        "    if info['unique_values'] is not None:\n",
        "        unique_vals = info['unique_values']\n",
        "        print(f\"Unique mask values: {unique_vals}\")\n",
        "\n",
        "        value_map = {int(old_val): new_val for new_val, old_val in enumerate(unique_vals)}\n",
        "        num_classes = len(unique_vals)\n",
        "\n",
        "        print(f\"\\nRemapping mask values:\")\n",
        "        for old_val, new_val in value_map.items():\n",
        "            print(f\"  {old_val} -> {new_val}\")\n",
        "        print(f\"\\nFinal number of classes: {num_classes}\")\n",
        "\n",
        "    # Step 2: Split dataset\n",
        "    train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
        "        info['image_files'],\n",
        "        info['mask_files'],\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain samples: {len(train_imgs)}\")\n",
        "    print(f\"Validation samples: {len(val_imgs)}\")\n",
        "\n",
        "    # Step 3: Create datasets\n",
        "    train_dataset = SegmentationDataset(\n",
        "        train_imgs, train_masks, num_classes, is_rgb_mask, IMG_SIZE, augment=True, value_map=value_map\n",
        "    )\n",
        "    val_dataset = SegmentationDataset(\n",
        "        val_imgs, val_masks, num_classes, is_rgb_mask, IMG_SIZE, augment=False, value_map=value_map\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    # VERIFY masks\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VERIFYING REMAPPED MASKS...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    sample_img, sample_mask = train_dataset[0]\n",
        "    unique_remapped = torch.unique(sample_mask)\n",
        "    print(f\"Sample mask shape: {sample_mask.shape}\")\n",
        "    print(f\"Unique values after remapping: {unique_remapped.numpy()}\")\n",
        "    print(f\"Min value: {sample_mask.min().item()}, Max value: {sample_mask.max().item()}\")\n",
        "\n",
        "    print(\"\\nChecking all masks in training set...\")\n",
        "    all_valid = True\n",
        "    for i in range(len(train_dataset)):\n",
        "        _, mask = train_dataset[i]\n",
        "        if mask.max().item() >= num_classes or mask.min().item() < 0:\n",
        "            print(f\"‚ö†Ô∏è  Invalid mask at index {i}: min={mask.min().item()}, max={mask.max().item()}\")\n",
        "            all_valid = False\n",
        "            if i > 10:\n",
        "                break\n",
        "\n",
        "    if not all_valid:\n",
        "        print(\"\\n‚ö†Ô∏è  ERROR: Some masks have invalid values!\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"‚úì All {len(train_dataset)} training masks are valid (0 to {num_classes-1})\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 4: Initialize FCN-8s model\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INITIALIZING FCN-8s MODEL...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        model = FCN8s(num_classes=num_classes, pretrained=True)\n",
        "\n",
        "        try:\n",
        "            model = model.to(device)\n",
        "            print(f\"‚úì FCN-8s model successfully moved to {device}\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"‚ö†Ô∏è  CUDA Error: {e}\")\n",
        "            print(\"üîÑ Falling back to CPU...\")\n",
        "            device = torch.device('cpu')\n",
        "            model = model.to(device)\n",
        "            print(f\"‚úì Model running on CPU\")\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        print(f\"Model initialized with {num_classes} classes\")\n",
        "        print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "        print(f\"Using pretrained VGG16 backbone\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Fatal error during model initialization: {e}\")\n",
        "        return\n",
        "\n",
        "    # Test forward pass\n",
        "    print(\"\\nTesting forward pass...\")\n",
        "    try:\n",
        "        dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
        "        dummy_output = model(dummy_input)\n",
        "        print(f\"‚úì Forward pass successful! Output shape: {dummy_output.shape}\")\n",
        "\n",
        "        dummy_target = torch.randint(0, num_classes, (1, 256, 256)).to(device)\n",
        "        dummy_loss = criterion(dummy_output, dummy_target)\n",
        "        print(f\"‚úì Loss calculation successful! Loss: {dummy_loss.item():.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error during forward pass: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 5: Training loop\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STARTING TRAINING WITH FCN-8s...\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "    print(f\"Image Size: {IMG_SIZE}\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, epoch+1, NUM_EPOCHS\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc = validate(\n",
        "            model, val_loader, criterion, device, epoch+1, NUM_EPOCHS\n",
        "        )\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        elapsed_time = time.time() - start_time\n",
        "        eta = (elapsed_time / (epoch + 1)) * (NUM_EPOCHS - epoch - 1)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Summary:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "        print(f\"  Time: {epoch_time:.1f}s | Elapsed: {elapsed_time/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_fcn8s_model.pth')\n",
        "            print(f\"  ‚úì Best model saved! (Val Loss: {best_val_loss:.4f}, Val Acc: {best_val_acc:.2f}%)\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"  GPU Memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
        "\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nüéâ Training completed in {total_time/60:.1f} minutes!\")\n",
        "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
        "    print(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Step 6: Plot training history\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', marker='o', markersize=3)\n",
        "    plt.plot(val_losses, label='Val Loss', marker='s', markersize=3)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('FCN-8s: Training & Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Acc', marker='o', markersize=3)\n",
        "    plt.plot(val_accs, label='Val Acc', marker='s', markersize=3)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('FCN-8s: Training & Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history_fcn8s.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Step 7: Visualize predictions\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"GENERATING PREDICTIONS...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model.load_state_dict(torch.load('best_fcn8s_model.pth'))\n",
        "    visualize_predictions(model, val_dataset, device, num_samples=4)\n",
        "\n",
        "    # Step 8: Calculate IoU\n",
        "    model.eval()\n",
        "    total_iou = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            total_iou += calculate_iou(preds, masks, num_classes)\n",
        "\n",
        "    mean_iou = total_iou / len(val_loader)\n",
        "    print(f\"\\nMean IoU: {mean_iou:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TRAINING COMPLETED!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Best model saved as: best_fcn8s_model.pth\")\n",
        "    print(f\"Training history saved as: training_history_fcn8s.png\")\n",
        "    print(f\"Predictions saved as: predictions_fcn8s.png\")\n",
        "\n",
        "# ==================== STEP 8: PREDICTION FUNCTION ====================\n",
        "def predict_single_image(model_path, image_path, num_classes, device):\n",
        "    \"\"\"Predict on a single image using FCN-8s\"\"\"\n",
        "    model = FCN8s(num_classes=num_classes, pretrained=False).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    original_size = image.size\n",
        "    image = image.resize((256, 256), Image.BILINEAR)\n",
        "    image_np = np.array(image) / 255.0\n",
        "    image_tensor = torch.FloatTensor(image_np).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    pred_resized = cv2.resize(pred.astype(np.uint8), original_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    axes[0].imshow(Image.open(image_path))\n",
        "    axes[0].set_title('Original Image', fontsize=16, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(pred_resized, cmap='tab20')\n",
        "    axes[1].set_title('FCN-8s Prediction', fontsize=16, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Overlay\n",
        "    overlay = np.array(Image.open(image_path)).copy()\n",
        "    mask_colored = plt.cm.tab20(pred_resized / max(num_classes-1, 1))[:, :, :3]\n",
        "    overlay = (overlay * 0.5 + mask_colored * 255 * 0.5).astype(np.uint8)\n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title('Overlay', fontsize=16, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('prediction_fcn8s.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return pred_resized\n",
        "\n",
        "# RUN TRAINING\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREDICT IMAGE"
      ],
      "metadata": {
        "id": "lFfeP5KicRNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# ==================== FCN-8s Architecture (Copied from previous cell) ====================\n",
        "class FCN8s(nn.Module):\n",
        "    def __init__(self, num_classes=21, pretrained=True):\n",
        "        super(FCN8s, self).__init__()\n",
        "\n",
        "        # Load pretrained VGG16\n",
        "        vgg16 = models.vgg16(pretrained=pretrained)\n",
        "        features = list(vgg16.features.children())\n",
        "\n",
        "        # Encoder (VGG16 feature extractor)\n",
        "        # Pool3: layers 0-16\n",
        "        self.pool3 = nn.Sequential(*features[:17])\n",
        "\n",
        "        # Pool4: layers 17-23\n",
        "        self.pool4 = nn.Sequential(*features[17:24])\n",
        "\n",
        "        # Pool5: layers 24-30\n",
        "        self.pool5 = nn.Sequential(*features[24:])\n",
        "\n",
        "        # FC6 - converted to conv\n",
        "        self.fc6 = nn.Conv2d(512, 4096, kernel_size=7, padding=3)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.drop6 = nn.Dropout2d()\n",
        "\n",
        "        # FC7 - converted to conv\n",
        "        self.fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.drop7 = nn.Dropout2d()\n",
        "\n",
        "        # Score layers\n",
        "        self.score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n",
        "        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "        self.score_pool3 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "        # Upsampling layers\n",
        "        self.upscore2 = nn.ConvTranspose2d(\n",
        "            num_classes, num_classes, kernel_size=4, stride=2, bias=False\n",
        "        )\n",
        "        self.upscore_pool4 = nn.ConvTranspose2d(\n",
        "            num_classes, num_classes, kernel_size=4, stride=2, bias=False\n",
        "        )\n",
        "        self.upscore8 = nn.ConvTranspose2d(\n",
        "            num_classes, num_classes, kernel_size=16, stride=8, bias=False\n",
        "        )\n",
        "\n",
        "        # Initialize upsampling with bilinear interpolation\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights for upsampling layers\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get input size for later cropping\n",
        "        # input_size = x.size()[2:] # Not directly used in FCN8s forward, but for cropping at the end\n",
        "\n",
        "        # Encoder\n",
        "        pool3 = self.pool3(x)  # 1/8\n",
        "        pool4 = self.pool4(pool3)  # 1/16\n",
        "        pool5 = self.pool5(pool4)  # 1/32\n",
        "\n",
        "        # FC layers as convolutions\n",
        "        fc6 = self.relu6(self.fc6(pool5))\n",
        "        fc6 = self.drop6(fc6)\n",
        "\n",
        "        fc7 = self.relu7(self.fc7(fc6))\n",
        "        fc7 = self.drop7(fc7)\n",
        "\n",
        "        # Score\n",
        "        score_fr = self.score_fr(fc7)\n",
        "\n",
        "        # Upsample score_fr (1/32 -> 1/16)\n",
        "        upscore2 = self.upscore2(score_fr)\n",
        "\n",
        "        # Score pool4\n",
        "        score_pool4 = self.score_pool4(pool4)\n",
        "\n",
        "        # Crop and add\n",
        "        upscore2 = self._crop(upscore2, score_pool4)\n",
        "        fuse_pool4 = upscore2 + score_pool4\n",
        "\n",
        "        # Upsample fuse_pool4 (1/16 -> 1/8)\n",
        "        upscore_pool4 = self.upscore_pool4(fuse_pool4)\n",
        "\n",
        "        # Score pool3\n",
        "        score_pool3 = self.score_pool3(pool3)\n",
        "\n",
        "        # Crop and add\n",
        "        upscore_pool4 = self._crop(upscore_pool4, score_pool3)\n",
        "        fuse_pool3 = upscore_pool4 + score_pool3\n",
        "\n",
        "        # Final upsampling (1/8 -> 1/1)\n",
        "        out = self.upscore8(fuse_pool3)\n",
        "\n",
        "        # Crop to input size\n",
        "        out = self._crop(out, x)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _crop(self, input_tensor, target_tensor):\n",
        "        \"\"\"Crop input_tensor to match target_tensor size\"\"\"\n",
        "        _, _, h, w = target_tensor.size()\n",
        "        return input_tensor[:, :, :h, :w]\n",
        "# ==================== AUTO-DETECT NUM_CLASSES ====================\n",
        "def detect_num_classes(model_path):\n",
        "    \"\"\"Auto-detect number of classes dari FCN-8s model\"\"\"\n",
        "    state_dict = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "    # FCN-8s classifier layer == score_fr.weight\n",
        "    for key in state_dict.keys():\n",
        "        if \"score_fr.weight\" in key:\n",
        "            return state_dict[key].shape[0]\n",
        "\n",
        "    raise ValueError(\"‚ùå Tidak menemukan score_fr.weight ‚Üí bukan FCN-8s?\")\n",
        "\n",
        "# ==================== PREDICT FUNCTION (FCN-8s) ====================\n",
        "def predict_image_fcn8s(model_path, image_path, num_classes=None):\n",
        "    \"\"\"Prediksi segmentasi menggunakan FCN-8s\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Auto detect jumlah kelas\n",
        "    if num_classes is None:\n",
        "        print(\"üîç Auto-detecting number of classes...\")\n",
        "        num_classes = detect_num_classes(model_path)\n",
        "        print(f\"‚úÖ Detected: {num_classes} classes\\n\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üîÆ PREDICTION MODE (FCN-8s)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Model: {model_path}\")\n",
        "    print(f\"Image: {image_path}\")\n",
        "    print(f\"Classes: {num_classes}\")\n",
        "    print(\"=\"*60, \"\\n\")\n",
        "\n",
        "    # Load model\n",
        "    print(\"üì¶ Loading FCN-8s model...\")\n",
        "    model = FCN8s(num_classes=num_classes, pretrained=False).to(device) # Set pretrained=False when loading a trained model\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded!\\n\")\n",
        "\n",
        "    # Load image\n",
        "    print(\"üñºÔ∏è  Loading image...\")\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    original_size = image.size\n",
        "    print(f\"Original size: {original_size[0]}x{original_size[1]}\")\n",
        "\n",
        "    # Preprocess\n",
        "    img_resized = image.resize((256, 256), Image.BILINEAR)\n",
        "    img_np = np.array(img_resized) / 255.0\n",
        "    img_tensor = torch.FloatTensor(img_np).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict\n",
        "    print(\"üöÄ Running inference...\")\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Resize mask ke ukuran asli\n",
        "    pred_resized = cv2.resize(pred.astype(np.uint8), original_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    unique_classes = np.unique(pred_resized)\n",
        "    print(\"‚úÖ Prediction completed!\")\n",
        "    print(f\"Detected classes: {unique_classes}\")\n",
        "    print(f\"Output shape: {pred_resized.shape}\\n\")\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=16, fontweight=\"bold\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    im = axes[1].imshow(pred_resized, cmap=\"tab20\", vmin=0, vmax=num_classes-1)\n",
        "    axes[1].set_title(\"Prediction Mask\", fontsize=16, fontweight=\"bold\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # Colorbar\n",
        "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "    divider = make_axes_locatable(axes[1])\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cbar = plt.colorbar(im, cax=cax)\n",
        "    cbar.set_label(\"Class ID\")\n",
        "\n",
        "    # Overlay\n",
        "    overlay = np.array(image).copy()\n",
        "    mask_color = plt.cm.tab20(pred_resized / max(num_classes-1, 1))[:, :, :3]\n",
        "    overlay = (overlay * 0.5 + mask_color * 255 * 0.5).astype(np.uint8)\n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title(\"Overlay\", fontsize=16, fontweight=\"bold\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"prediction_fcn8s_result.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    print(\"üíæ Result saved to: prediction_fcn8s_result.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Statistik\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä PREDICTION STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "    for class_id in range(num_classes):\n",
        "        pixels = np.sum(pred_resized == class_id)\n",
        "        percentage = (pixels / pred_resized.size) * 100\n",
        "        if pixels > 0:\n",
        "            print(f\"Class {class_id:2d}: {pixels:8,} pixels ({percentage:6.2f}%) Kishan\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚ú® DONE!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return pred_resized\n",
        "FCN_MODEL_PATH = '/content/drive/MyDrive/best_fcn8s_model.pth'\n",
        "FCN_IMAGE_PATH = '/content/drive/MyDrive/download (1).jpg'\n",
        "\n",
        "prediction = predict_image_fcn8s(FCN_MODEL_PATH, FCN_IMAGE_PATH)"
      ],
      "metadata": {
        "id": "gJvuDRFCcGzv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}